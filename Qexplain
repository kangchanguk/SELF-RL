exploit exploration: E-greedy

e=0.1

if rand<e:
    a=random
else:
    a=argmax(q)

랜덤한 값이 e보다 적다면 랜덤하게 행동선택, e보다 크거나 같다면 최선책을 선택

for i in range(1000):
    e=0.1/(i+1)
    if rand<e:
        a=random
    else:
        a=argmax(q)

add random noise

0.5, 0.6, 0.3, 0.2 0.5

노이즈를 섞어서 최선책을 선택한다.

a= argmax(q + random noise)- 변해진 값에 의거 가장 좋은 식당을 찾는다!!!


select an action a and execute it( exploit and exploration)을 통해 선택
단 랜덤하게 행동 선택시 agent가 헷갈리는 상황이 발생한다.

Q(S,A)=R+ MAX(S',A')-미래의 AWARD는 현재보다 좋지 않기에 DISCOUNT를 한다.
FUTURE REWARD R=R1+R2+R3+R4+R5+R6
감소율을 거듭 제곱으로 곱해가면서 미래의 보상은 점점 크기가 줄어든다.



Q(S,A)=R+감소율 x MAX Q(S',A')

q테이블을 채워 넣는 것이 학습 이고 ARGMAX( q(S,A) )





